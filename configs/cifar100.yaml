model_name: "vit_base_patch16_224"
dataset: "cifar100"
data_dir: "./data"
image_size: 224
batch_size: 16
num_workers: 4

# Training parameters
epochs: 20
learning_rate: 0.00005
weight_decay: 0.01
optimizer: "adamw"
scheduler: "cosine"

# LoRA parameters
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.1
target_modules: ["query", "value", "key", "dense"]

# Performance monitoring
warmup_iterations: 5
benchmark_iterations: 100
save_plots: true
plot_dir: "./plots"

# Model saving
save_model: true
model_dir: "./models"

# Device settings
device: "cuda"
mixed_precision: true

# Logging
log_interval: 50
use_wandb: false
project_name: "vitscale"
