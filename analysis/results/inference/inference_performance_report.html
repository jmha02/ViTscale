
    <!DOCTYPE html>
    <html>
    <head>
        <title>ViT Inference Performance Analysis Report</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }
            .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; }
            .section { margin: 30px 0; padding: 20px; border: 1px solid #ddd; border-radius: 8px; }
            .highlight { background-color: #f0f8ff; padding: 15px; border-left: 4px solid #007acc; }
            .metric { display: inline-block; margin: 10px; padding: 15px; background: #f9f9f9; border-radius: 5px; }
            .best { background-color: #e8f5e8; border-left: 4px solid #4caf50; }
            table { width: 100%; border-collapse: collapse; margin: 20px 0; }
            th, td { padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }
            th { background-color: #f2f2f2; font-weight: bold; }
            .improvement { color: #4caf50; font-weight: bold; }
            .degradation { color: #f44336; font-weight: bold; }
            img { max-width: 100%; height: auto; margin: 20px 0; border: 1px solid #ddd; }
        </style>
    </head>
    <body>
        <div class="header">
            <h1>‚ö° ViT Inference Performance Analysis Report</h1>
            <p>Comprehensive analysis of Vision Transformer inference performance with LoRA adaptations</p>
            <p><strong>Generated:</strong> 2025-06-20 04:09:36</p>
        </div>

        <div class="section">
            <h2>üìä Executive Summary</h2>
            <div class="highlight">
                <p><strong>Key Finding:</strong> LoRA shows trade-offs in inference: parameter efficiency gains with slight performance overhead.</p>
                <p><strong>Best Overall Model:</strong> LoRA-8 with 2.68 FPS per million parameters</p>
            </div>
        </div>

        <div class="section">
            <h2>üèÅ Baseline Performance</h2>
            <div class="metric">
                <strong>Inference Time:</strong> 4.62 ms per sample
            </div>
            <div class="metric">
                <strong>Throughput:</strong> 216.5 FPS
            </div>
            <div class="metric">
                <strong>Memory:</strong> 453 MB
            </div>
            <div class="metric">
                <strong>FLOPs:</strong> 0.0 GFLOPs
            </div>
            <div class="metric">
                <strong>Parameters:</strong> 85,806,346 (100% trainable)
            </div>
        </div>

        <div class="section">
            <h2>‚ö° LoRA Performance Comparison</h2>
            <table>
                <tr>
                    <th>Model</th>
                    <th>Inference Time (ms)</th>
                    <th>Speedup</th>
                    <th>Throughput (FPS)</th>
                    <th>Memory (MB)</th>
                    <th>FLOPs (G)</th>
                    <th>Trainable Params</th>
                    <th>Efficiency</th>
                </tr>
                <tr>
                    <td><strong>LoRA-4</strong></td>
                    <td>6.66</td>
                    <td class="degradation">0.69x</td>
                    <td>150.1</td>
                    <td>781 <span class="degradation">(+72.5%)</span></td>
                    <td>0.0</td>
                    <td>57,679,114 <span class="improvement">(-32.8%)</span></td>
                    <td>2.60</td>
                </tr>
                <tr>
                    <td><strong>LoRA-8</strong></td>
                    <td>6.44</td>
                    <td class="degradation">0.72x</td>
                    <td>155.3</td>
                    <td>782 <span class="degradation">(+72.8%)</span></td>
                    <td>0.0</td>
                    <td>57,900,298 <span class="improvement">(-32.5%)</span></td>
                    <td>2.68</td>
                </tr>
                <tr>
                    <td><strong>LoRA-16</strong></td>
                    <td>6.66</td>
                    <td class="degradation">0.69x</td>
                    <td>150.2</td>
                    <td>784 <span class="degradation">(+73.2%)</span></td>
                    <td>0.0</td>
                    <td>58,342,666 <span class="improvement">(-32.0%)</span></td>
                    <td>2.58</td>
                </tr>
                <tr>
                    <td><strong>LoRA-32</strong></td>
                    <td>6.62</td>
                    <td class="degradation">0.70x</td>
                    <td>151.0</td>
                    <td>788 <span class="degradation">(+74.0%)</span></td>
                    <td>0.0</td>
                    <td>59,227,402 <span class="improvement">(-31.0%)</span></td>
                    <td>2.55</td>
                </tr>
                <tr>
                    <td><strong>LoRA-64</strong></td>
                    <td>6.59</td>
                    <td class="degradation">0.70x</td>
                    <td>151.8</td>
                    <td>795 <span class="degradation">(+75.6%)</span></td>
                    <td>0.0</td>
                    <td>60,996,874 <span class="improvement">(-28.9%)</span></td>
                    <td>2.49</td>
                </tr>
            </table>
        </div>

        <div class="section">
            <h2>üèÜ Best Performers</h2>
            <div class="best">
                <h3>ü•á Fastest Inference: LoRA-8</h3>
                <p>Inference time: 6.44 ms (0.72x speedup)</p>
            </div>
            <div class="best">
                <h3>ü•á Most Efficient: LoRA-8</h3>
                <p>Efficiency: 2.68 FPS per million parameters</p>
            </div>
            <div class="best">
                <h3>ü•á Lowest Memory: LoRA-4</h3>
                <p>Memory usage: 781 MB</p>
            </div>
        </div>

        <div class="section">
            <h2>üìà Analysis Visualizations</h2>
            <h3>Inference Performance Overview</h3>
            <img src="inference_performance_overview.png" alt="Inference Performance Overview">
            
            <h3>Parameter Efficiency Analysis</h3>
            <img src="inference_parameter_efficiency.png" alt="Parameter Efficiency Analysis">
            
            <h3>LoRA Rank Impact Analysis</h3>
            <img src="lora_rank_analysis.png" alt="LoRA Rank Analysis">
        </div>

        <div class="section">
            <h2>üîç Key Insights</h2>
            <ul>
                <li><strong>Parameter Efficiency:</strong> LoRA models achieve ~32% parameter reduction</li>
                <li><strong>Inference Overhead:</strong> LoRA introduces slight computational overhead during inference</li>
                <li><strong>Memory Impact:</strong> LoRA models use more memory due to additional layer structures</li>
                <li><strong>Trade-off:</strong> Parameter efficiency comes at the cost of slight inference speed reduction</li>
            </ul>
        </div>

        <div class="section">
            <h2>üí° Recommendations</h2>
            <ul>
                <li><strong>For fastest inference:</strong> Use baseline model for production inference</li>
                <li><strong>For parameter efficiency:</strong> Use LoRA-8 configuration</li>
                <li><strong>For deployment constraints:</strong> Lower LoRA ranks provide better inference efficiency</li>
                <li><strong>Best practice:</strong> Train with LoRA, fine-tune and merge weights for inference deployment</li>
            </ul>
        </div>
    </body>
    </html>
    