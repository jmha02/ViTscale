
    <!DOCTYPE html>
    <html>
    <head>
        <title>ViT Training Performance Analysis Report</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }
            .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; }
            .section { margin: 30px 0; padding: 20px; border: 1px solid #ddd; border-radius: 8px; }
            .highlight { background-color: #f0f8ff; padding: 15px; border-left: 4px solid #007acc; }
            .metric { display: inline-block; margin: 10px; padding: 15px; background: #f9f9f9; border-radius: 5px; }
            .best { background-color: #e8f5e8; border-left: 4px solid #4caf50; }
            table { width: 100%; border-collapse: collapse; margin: 20px 0; }
            th, td { padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }
            th { background-color: #f2f2f2; font-weight: bold; }
            .improvement { color: #4caf50; font-weight: bold; }
            .degradation { color: #f44336; font-weight: bold; }
            img { max-width: 100%; height: auto; margin: 20px 0; border: 1px solid #ddd; }
        </style>
    </head>
    <body>
        <div class="header">
            <h1>üöÄ ViT Training Performance Analysis Report</h1>
            <p>Comprehensive analysis of Vision Transformer training performance with LoRA adaptations</p>
            <p><strong>Generated:</strong> 2025-06-20 04:11:06</p>
        </div>

        <div class="section">
            <h2>üìä Executive Summary</h2>
            <div class="highlight">
                <p><strong>Key Finding:</strong> LoRA demonstrates significant parameter efficiency gains in training while maintaining competitive performance.</p>
                <p><strong>Best Overall Model:</strong> LoRA-4 with 2.06 samples/sec per million parameters</p>
            </div>
        </div>

        <div class="section">
            <h2>üèÅ Baseline Performance</h2>
            <div class="metric">
                <strong>Training Time:</strong> 271.64 ms per batch
            </div>
            <div class="metric">
                <strong>Throughput:</strong> 117.8 samples/sec
            </div>
            <div class="metric">
                <strong>Memory:</strong> 4720 MB
            </div>
            <div class="metric">
                <strong>Parameters:</strong> 85,806,346 (100% trainable)
            </div>
        </div>

        <div class="section">
            <h2>‚ö° LoRA Performance Comparison</h2>
            <table>
                <tr>
                    <th>Model</th>
                    <th>Training Time (ms)</th>
                    <th>Speedup</th>
                    <th>Throughput (samples/sec)</th>
                    <th>Memory (MB)</th>
                    <th>Trainable Params</th>
                    <th>Efficiency</th>
                </tr>
                <tr>
                    <td><strong>LoRA-4</strong></td>
                    <td>269.38</td>
                    <td class="improvement">1.01x</td>
                    <td>118.8</td>
                    <td>4519 <span class="improvement">(-4.3%)</span></td>
                    <td>57,679,114 <span class="improvement">(-32.8%)</span></td>
                    <td>2.06</td>
                </tr>
                <tr>
                    <td><strong>LoRA-8</strong></td>
                    <td>269.45</td>
                    <td class="improvement">1.01x</td>
                    <td>118.8</td>
                    <td>4524 <span class="improvement">(-4.1%)</span></td>
                    <td>57,900,298 <span class="improvement">(-32.5%)</span></td>
                    <td>2.05</td>
                </tr>
                <tr>
                    <td><strong>LoRA-16</strong></td>
                    <td>269.82</td>
                    <td class="improvement">1.01x</td>
                    <td>118.6</td>
                    <td>4535 <span class="improvement">(-3.9%)</span></td>
                    <td>58,342,666 <span class="improvement">(-32.0%)</span></td>
                    <td>2.03</td>
                </tr>
                <tr>
                    <td><strong>LoRA-32</strong></td>
                    <td>270.27</td>
                    <td class="improvement">1.01x</td>
                    <td>118.4</td>
                    <td>4556 <span class="improvement">(-3.5%)</span></td>
                    <td>59,227,402 <span class="improvement">(-31.0%)</span></td>
                    <td>2.00</td>
                </tr>
                <tr>
                    <td><strong>LoRA-64</strong></td>
                    <td>273.57</td>
                    <td class="degradation">0.99x</td>
                    <td>117.0</td>
                    <td>4610 <span class="improvement">(-2.3%)</span></td>
                    <td>60,996,874 <span class="improvement">(-28.9%)</span></td>
                    <td>1.92</td>
                </tr>
            </table>
        </div>

        <div class="section">
            <h2>üèÜ Best Performers</h2>
            <div class="best">
                <h3>ü•á Fastest Training: LoRA-4</h3>
                <p>Training time: 269.38 ms (1.01x speedup)</p>
            </div>
            <div class="best">
                <h3>ü•á Most Efficient: LoRA-4</h3>
                <p>Efficiency: 2.06 samples/sec per million parameters</p>
            </div>
            <div class="best">
                <h3>ü•á Lowest Memory: LoRA-4</h3>
                <p>Memory usage: 4519 MB</p>
            </div>
        </div>

        <div class="section">
            <h2>üìà Analysis Visualizations</h2>
            <h3>Training Performance Overview</h3>
            <img src="training_performance_overview.png" alt="Training Performance Overview">
            
            <h3>Parameter Efficiency Analysis</h3>
            <img src="training_parameter_efficiency.png" alt="Parameter Efficiency Analysis">
            
            <h3>LoRA Rank Impact Analysis</h3>
            <img src="lora_rank_analysis.png" alt="LoRA Rank Analysis">
        </div>

        <div class="section">
            <h2>üîç Key Insights</h2>
            <ul>
                <li><strong>Parameter Efficiency:</strong> LoRA models achieve ~32% parameter reduction while maintaining competitive training speed</li>
                <li><strong>Memory Efficiency:</strong> LoRA models use ~3.6% less memory on average</li>
                <li><strong>Training Speed:</strong> LoRA models show slight training speedup due to reduced parameter updates</li>
                <li><strong>Sweet Spot:</strong> Lower LoRA ranks (4-8) provide the best efficiency without significant performance loss</li>
            </ul>
        </div>

        <div class="section">
            <h2>üí° Recommendations</h2>
            <ul>
                <li><strong>For fastest training:</strong> Use LoRA-4 configuration</li>
                <li><strong>For best efficiency:</strong> Use LoRA-4 configuration</li>
                <li><strong>For memory constraints:</strong> Use LoRA-4 configuration</li>
                <li><strong>General use:</strong> LoRA-8 provides excellent balance of speed, memory, and parameter efficiency</li>
            </ul>
        </div>
    </body>
    </html>
    